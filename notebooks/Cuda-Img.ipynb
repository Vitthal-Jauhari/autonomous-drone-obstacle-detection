{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0930b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4486d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDepthAnythingV2:\n",
    "    def __init__(self, encoder='vits', device=None):\n",
    "        \"\"\"\n",
    "        Initialize the optimized Depth Anything V2 model\n",
    "        \n",
    "        Args:\n",
    "            encoder: Model size ('vits', 'vitb', 'vitl', 'vitg')\n",
    "            device: Device to run on (auto-detected if None)\n",
    "        \"\"\"\n",
    "        # Auto-detect best available device\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device('cuda')\n",
    "                print(f\"Using CUDA: {torch.cuda.get_device_name()}\")\n",
    "            elif torch.backends.mps.is_available():\n",
    "                self.device = torch.device('mps')\n",
    "                print(\"Using Apple Metal Performance Shaders (MPS)\")\n",
    "            else:\n",
    "                self.device = torch.device('cpu')\n",
    "                print(\"Using CPU\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        # Model configurations\n",
    "        self.model_configs = {\n",
    "            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "            'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "        }\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.model = self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load and initialize the model\"\"\"\n",
    "        print(f\"Loading {self.encoder} model...\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DepthAnythingV2(**self.model_configs[self.encoder])\n",
    "        \n",
    "        # Load weights\n",
    "        checkpoint_path = f'checkpoints/depth_anything_v2_{self.encoder}.pth'\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            \n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location='cpu', weights_only=True))\n",
    "        \n",
    "        # Move to device and set to eval mode\n",
    "        model = model.to(self.device).eval()\n",
    "        \n",
    "        # Enable optimizations for inference\n",
    "        if self.device.type == 'cuda':\n",
    "            model = model.half()  # Use FP16 for faster inference on modern GPUs\n",
    "            torch.backends.cudnn.benchmark = True  # Optimize CUDNN for consistent input sizes\n",
    "            \n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "        return model\n",
    "        \n",
    "    def warmup(self, input_size=518, num_warmup=3):\n",
    "        \"\"\"Warm up the model for consistent timing\"\"\"\n",
    "        print(\"Warming up model...\")\n",
    "        dummy_input = torch.randn(1, 3, input_size, input_size, device=self.device)\n",
    "        if self.device.type == 'cuda':\n",
    "            dummy_input = dummy_input.half()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = self.model(dummy_input)\n",
    "                \n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.synchronize()  # Ensure all operations complete\n",
    "        print(\"Warmup complete\")\n",
    "        \n",
    "    def infer_image(self, raw_image, input_size=384):\n",
    "        \"\"\"\n",
    "        Optimized inference with proper error handling\n",
    "        \n",
    "        Args:\n",
    "            raw_image: Input image (BGR format from cv2.imread)\n",
    "            input_size: Model input size\n",
    "            \n",
    "        Returns:\n",
    "            depth: Depth map as numpy array\n",
    "            inference_time: Time taken for inference in seconds\n",
    "        \"\"\"\n",
    "        if raw_image is None:\n",
    "            raise ValueError(\"Input image is None\")\n",
    "            \n",
    "        original_height, original_width = raw_image.shape[:2]\n",
    "        \n",
    "        # Preprocessing - convert BGR to RGB and normalize\n",
    "        image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert to tensor and resize\n",
    "        image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n",
    "        image_tensor = F.interpolate(\n",
    "            image_tensor, \n",
    "            (input_size, input_size), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Normalize with ImageNet stats\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        image_tensor = (image_tensor - mean) / std\n",
    "        \n",
    "        # Move to device and convert to appropriate dtype\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        if self.device.type == 'cuda':\n",
    "            image_tensor = image_tensor.half()\n",
    "            \n",
    "        # Inference with timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            depth = self.model(image_tensor)\n",
    "            \n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.synchronize()  # Ensure GPU operations complete\n",
    "            \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Post-processing - resize back to original dimensions\n",
    "        depth = F.interpolate(\n",
    "            depth, \n",
    "            (original_height, original_width), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        depth = depth.squeeze().cpu().numpy()\n",
    "        \n",
    "        return depth, inference_time\n",
    "    \n",
    "    def process_single_image(self, image_path, output_dir='./vis_depth', \n",
    "                           input_size=518, grayscale=False, pred_only=True):\n",
    "        \"\"\"Process a single image with optimized pipeline\"\"\"\n",
    "        \n",
    "        # Read image\n",
    "        raw_image = cv2.imread(image_path)\n",
    "        if raw_image is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "            \n",
    "        # Run inference\n",
    "        depth, inference_time = self.infer_image(raw_image, input_size)\n",
    "        \n",
    "        # Normalize depth for visualization\n",
    "        depth_normalized = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "        depth_normalized = depth_normalized.astype(np.uint8)\n",
    "        \n",
    "        # Apply colormap or grayscale\n",
    "        if grayscale:\n",
    "            depth_vis = np.repeat(depth_normalized[..., np.newaxis], 3, axis=-1)\n",
    "        else:\n",
    "            cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "            depth_vis = (cmap(depth_normalized)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)\n",
    "        \n",
    "        # Save output\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        if pred_only:\n",
    "            output_path = os.path.join(output_dir, f'{filename}_depth.png')\n",
    "            cv2.imwrite(output_path, depth_vis)\n",
    "        else:\n",
    "            # Create side-by-side comparison\n",
    "            separator = np.ones((raw_image.shape[0], 50, 3), dtype=np.uint8) * 255\n",
    "            combined = cv2.hconcat([raw_image, separator, depth_vis])\n",
    "            output_path = os.path.join(output_dir, f'{filename}_combined.png')\n",
    "            cv2.imwrite(output_path, combined)\n",
    "            \n",
    "        print(f\"Processed {os.path.basename(image_path)} in {inference_time*1000:.2f}ms\")\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        \n",
    "        return depth, inference_time\n",
    "    \n",
    "    def benchmark(self, image_path, num_runs=10, input_size=518):\n",
    "        \"\"\"Benchmark the model performance\"\"\"\n",
    "        raw_image = cv2.imread(image_path)\n",
    "        if raw_image is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "            \n",
    "        print(f\"Benchmarking with {num_runs} runs...\")\n",
    "        \n",
    "        # Warmup\n",
    "        self.warmup()\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        for i in range(num_runs):\n",
    "            _, inference_time = self.infer_image(raw_image, input_size)\n",
    "            times.append(inference_time)\n",
    "            print(f\"Run {i+1}/{num_runs}: {inference_time*1000:.2f}ms\")\n",
    "            \n",
    "        # Statistics\n",
    "        times = np.array(times)\n",
    "        print(f\"\\nBenchmark Results:\")\n",
    "        print(f\"Mean: {times.mean()*1000:.2f}ms\")\n",
    "        print(f\"Std:  {times.std()*1000:.2f}ms\")\n",
    "        print(f\"Min:  {times.min()*1000:.2f}ms\")\n",
    "        print(f\"Max:  {times.max()*1000:.2f}ms\")\n",
    "        \n",
    "        return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740707a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Loading vits model...\n",
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = OptimizedDepthAnythingV2(encoder='vits')  # or 'vitb', 'vitl', 'vitg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7897b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example single image processing\n",
    "image_path = r\"C:\\Codes\\Python\\obstacle_detection\\Depth-Anything-V2\\batch_input\\right10.jpg\"  # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0e2a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up model...\n",
      "Warmup complete\n",
      "Processed right10.jpg in 44.83ms\n",
      "Saved to: ./cuda_depth_output\\right10_depth.png\n",
      "Depth shape: (3072, 4096)\n",
      "Inference time: 44.83ms\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Process single image\n",
    "        model.warmup()\n",
    "        depth, inference_time = model.process_single_image(\n",
    "            image_path=image_path,\n",
    "            output_dir='./cuda_depth_output',\n",
    "            pred_only=True,\n",
    "            grayscale=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Depth shape: {depth.shape}\")\n",
    "        print(f\"Inference time: {inference_time*1000:.2f}ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571a6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with 10 runs...\n",
      "Warming up model...\n",
      "Warmup complete\n",
      "Run 1/10: 40.10ms\n",
      "Run 2/10: 44.18ms\n",
      "Run 3/10: 44.54ms\n",
      "Run 4/10: 44.72ms\n",
      "Run 5/10: 44.81ms\n",
      "Run 6/10: 47.62ms\n",
      "Run 7/10: 42.89ms\n",
      "Run 8/10: 45.02ms\n",
      "Run 9/10: 44.52ms\n",
      "Run 10/10: 45.17ms\n",
      "\n",
      "Benchmark Results:\n",
      "Mean: 44.36ms\n",
      "Std:  1.80ms\n",
      "Min:  40.10ms\n",
      "Max:  47.62ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04010439, 0.04417849, 0.04454017, 0.04471803, 0.04480529,\n",
       "       0.047616  , 0.04288912, 0.04502106, 0.04451895, 0.04517365])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Run benchmark\n",
    "model.benchmark(image_path, num_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831c68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
