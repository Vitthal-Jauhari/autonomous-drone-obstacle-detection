{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae54a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8246901-8c27-4916-ab05-b364959218da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7f7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraOptimizedDepthAnythingV2:\n",
    "    def __init__(self, encoder='vits', device=None):\n",
    "        \"\"\"Ultra-optimized version targeting sub-30ms on RTX 3050\"\"\"\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        # Model configurations\n",
    "        self.model_configs = {\n",
    "            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "        }\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.model = self._load_model()\n",
    "        \n",
    "        # Pre-compute normalization tensors\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n",
    "        \n",
    "        # Pre-allocate tensors for common sizes to avoid memory allocation overhead\n",
    "        self._tensor_cache = {}\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model with maximum optimizations\"\"\"\n",
    "        print(f\"Loading {self.encoder} model with ultra optimizations...\")\n",
    "        \n",
    "        model = DepthAnythingV2(**self.model_configs[self.encoder])\n",
    "        \n",
    "        checkpoint_path = f'checkpoints/depth_anything_v2_{self.encoder}.pth'\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location='cpu', weights_only=True))\n",
    "        model = model.to(self.device).eval()\n",
    "        \n",
    "        if self.device.type == 'cuda':\n",
    "            # Maximum performance settings\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            \n",
    "            # Enable graph capture for even faster inference\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Compile with aggressive optimization\n",
    "            try:\n",
    "                model = torch.compile(model, mode='max-autotune', dynamic=False)\n",
    "                print(\"Model compiled with maximum optimization\")\n",
    "            except:\n",
    "                try:\n",
    "                    model = torch.compile(model, mode='reduce-overhead')\n",
    "                    print(\"Model compiled with reduce-overhead mode\")\n",
    "                except:\n",
    "                    print(\"Compilation not available, using standard optimization\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _get_cached_tensor(self, shape, dtype=torch.float32):\n",
    "        \"\"\"Get pre-allocated tensor to avoid memory allocation overhead\"\"\"\n",
    "        key = (shape, dtype)\n",
    "        if key not in self._tensor_cache:\n",
    "            self._tensor_cache[key] = torch.empty(shape, device=self.device, dtype=dtype)\n",
    "        return self._tensor_cache[key]\n",
    "    \n",
    "    def infer_image_ultra_fast(self, raw_image, input_size=518):\n",
    "        \"\"\"Ultra-fast inference targeting sub-30ms\"\"\"\n",
    "        \n",
    "        original_height, original_width = raw_image.shape[:2]\n",
    "        \n",
    "        with torch.cuda.device(self.device):\n",
    "            # Use pre-allocated tensor if possible\n",
    "            input_tensor = self._get_cached_tensor((1, 3, input_size, input_size))\n",
    "            \n",
    "            # Direct GPU upload and processing\n",
    "            image_gpu = torch.from_numpy(raw_image).to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Optimized preprocessing pipeline\n",
    "            image_gpu = image_gpu.flip(-1).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            image_gpu.div_(255.0)  # In-place division\n",
    "            \n",
    "            # Resize with minimal overhead\n",
    "            F.interpolate(image_gpu, (input_size, input_size), \n",
    "                         mode='bilinear', align_corners=False, \n",
    "                         antialias=False, out=input_tensor)\n",
    "            \n",
    "            # Normalize in-place\n",
    "            input_tensor.sub_(self.mean).div_(self.std)\n",
    "            \n",
    "            # Inference timing\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                depth = self.model(input_tensor)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = time.perf_counter() - start_time\n",
    "            \n",
    "            # Fast resize back\n",
    "            depth = F.interpolate(depth, (original_height, original_width), \n",
    "                                mode='bilinear', align_corners=False, antialias=False)\n",
    "            \n",
    "            depth = depth.squeeze().cpu().numpy()\n",
    "            \n",
    "        return depth, inference_time\n",
    "    \n",
    "    def warmup_ultra(self, input_size=518, num_runs=10):\n",
    "        \"\"\"Extended warmup for ultra-stable performance\"\"\"\n",
    "        print(f\"Ultra warmup with {num_runs} runs...\")\n",
    "        \n",
    "        dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Progressive warmup\n",
    "        for i in range(num_runs):\n",
    "            _, time_taken = self.infer_image_ultra_fast(dummy_image, input_size)\n",
    "            print(f\"Warmup {i+1}/{num_runs}: {time_taken*1000:.2f}ms\")\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Ultra warmup complete\")\n",
    "    \n",
    "    def benchmark_ultra(self, image_path, num_runs=20):\n",
    "        \"\"\"Extended benchmark for statistical accuracy\"\"\"\n",
    "        raw_image = cv2.imread(image_path)\n",
    "        if raw_image is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "            \n",
    "        print(f\"Ultra benchmark with {num_runs} runs...\")\n",
    "        \n",
    "        # Extended warmup first\n",
    "        self.warmup_ultra(num_runs=15)\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        for i in range(num_runs):\n",
    "            _, inference_time = self.infer_image_ultra_fast(raw_image)\n",
    "            times.append(inference_time * 1000)  # Convert to ms\n",
    "            print(f\"Run {i+1:2d}/{num_runs}: {inference_time*1000:5.2f}ms\")\n",
    "            \n",
    "        times = np.array(times)\n",
    "        \n",
    "        # Detailed statistics\n",
    "        print(f\"\\nðŸ“Š ULTRA BENCHMARK RESULTS:\")\n",
    "        print(f\"{'Mean:':<12} {times.mean():5.2f}ms\")\n",
    "        print(f\"{'Median:':<12} {np.median(times):5.2f}ms\")\n",
    "        print(f\"{'Std Dev:':<12} {times.std():5.2f}ms\")\n",
    "        print(f\"{'Min:':<12} {times.min():5.2f}ms\")\n",
    "        print(f\"{'Max:':<12} {times.max():5.2f}ms\")\n",
    "        print(f\"{'95th %ile:':<12} {np.percentile(times, 95):5.2f}ms\")\n",
    "        print(f\"{'5th %ile:':<12} {np.percentile(times, 5):5.2f}ms\")\n",
    "        \n",
    "        # Performance classification\n",
    "        mean_time = times.mean()\n",
    "        if mean_time < 30:\n",
    "            print(\"ðŸš€ EXCELLENT: Sub-30ms performance!\")\n",
    "        elif mean_time < 50:\n",
    "            print(\"âœ… GREAT: Sub-50ms performance!\")\n",
    "        elif mean_time < 100:\n",
    "            print(\"ðŸ‘ GOOD: Sub-100ms performance\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Could be optimized further\")\n",
    "            \n",
    "        return times\n",
    "\n",
    "\n",
    "def compare_input_sizes(model, image_path):\n",
    "    \"\"\"Test different input sizes for speed vs accuracy\"\"\"\n",
    "    sizes = [384, 448, 518, 588]\n",
    "    \n",
    "    print(\"ðŸ”¬ INPUT SIZE COMPARISON:\")\n",
    "    print(\"Size  | Time (ms) | Speed Gain\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    baseline_time = None\n",
    "    \n",
    "    for size in sizes:\n",
    "        raw_image = cv2.imread(image_path)\n",
    "        \n",
    "        # Warm up for this size\n",
    "        for _ in range(3):\n",
    "            model.infer_image_ultra_fast(raw_image, size)\n",
    "            \n",
    "        # Benchmark this size\n",
    "        times = []\n",
    "        for _ in range(10):\n",
    "            _, t = model.infer_image_ultra_fast(raw_image, size)\n",
    "            times.append(t * 1000)\n",
    "            \n",
    "        avg_time = np.mean(times)\n",
    "        \n",
    "        if baseline_time is None:\n",
    "            baseline_time = avg_time\n",
    "            \n",
    "        speed_gain = baseline_time / avg_time\n",
    "        print(f\"{size:4d}  | {avg_time:7.2f}   | {speed_gain:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93873536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vits model with ultra optimizations...\n",
      "ðŸš€ ULTRA-OPTIMIZED DEPTH ANYTHING V2\n",
      "==================================================\n",
      "Ultra benchmark with 20 runs...\n",
      "Ultra warmup with 15 runs...\n",
      "Error: Expected a cuda device, but got: cpu\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Ultra-optimized testing\"\"\"\n",
    "    model = UltraOptimizedDepthAnythingV2(encoder='vits')\n",
    "    \n",
    "    image_path = r\"C:\\Codes\\Python\\obstacle_detection\\Depth-Anything-V2\\batch_input\\left15.jpg\"   # Update this\n",
    "    \n",
    "    try:\n",
    "        print(\"ðŸš€ ULTRA-OPTIMIZED DEPTH ANYTHING V2\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extended benchmark\n",
    "        times = model.benchmark_ultra(image_path, num_runs=20)\n",
    "        \n",
    "        # Test different input sizes\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        compare_input_sizes(model, image_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b790d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
